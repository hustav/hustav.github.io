---
layout: archive
title: "My Hallucination Papers"
permalink: /hallucination/
author_profile: true
---

I did not write these scientific papers—though, according to ChatGPT, I could have.

I admire Google Scholar’s efficiency in crawling the scientific literature, automatically identifying our papers and their citations. It feels like magic and sets it apart from others that often require manual reference entry, have limited coverage of top conferences in my area, or miss recent preprints.

However, I’ve noticed something unusual recently. Google Scholar has increasingly suggested that I authored papers that I never wrote. But why are these nonexistent papers suddenly appearing on my profile?

As you might have guessed, these ghost papers are likely hallucinations of ChatGPT and other large language models (LLMs).

These citations appear in questionable literature reviews published in dubious journals by profit-driven publishers. The pattern is clear: these papers, published in 2023 or later (post-LLM boom), suddenly cite my supposedly co-authored paper from years ago—one that never existed.

Take this citation:

[Batista, G. E., & Monard, M. C. (2018). Data quality in machine learning: A study in the context of imbalanced data. Neurocomputing, 275, 1665-1679.](https://scholar.google.com/scholar?cites=3788517226534024053&as_sdt=2005&sciodt=0,5&hl=en)

This paper was never written. In fact, my esteemed PhD supervisor, Prof. Monard, had long retired by 2018. Yet, this fabricated reference appeared for the first time in a well-cited MDPI paper:

[Aldoseri, A., Al-Khalifa, K. N., & Hamouda, A. M. (2023). Re-thinking data strategy and integration for artificial intelligence: concepts, opportunities, and challenges. Applied Sciences, 13(12), 7082.](https://www.mdpi.com/2076-3417/13/12/7082)

Unsurprisingly, other papers citing my nonexistent work also cite Aldoseri et al., illustrating how bad science spreads like a disease. For example:

[Heudel, P. E., Crochet, H., & Blay, J. Y. (2024). Impact of artificial intelligence in transforming the doctor–cancer patient relationship. ESMO Real World Data and Digital Oncology, 3, 100026.](https://www.sciencedirect.com/science/article/pii/S2949820124000043)

Another fabricated reference:

[Batista, G. E., Prati, R. C., & Monard, M. C. (2004). Class Imbalance Problem in Data Mining: Review. ACM SIGKDD Explorations Newsletter, 6(1), 1-10.](https://scholar.google.com/scholar?cluster=3699090286396165465&hl=en)

This bibliographic hallucination originated from yet another MDPI paper:

[Buzea, C. G., Buga, R., Paun, M. A., Albu, M., Iancu, D. T., Dobrovat, B., … & Eva, L. (2023). AI evaluation of imaging factors in the evolution of stage-treated metastases using Gamma Knife. Diagnostics, 13(17), 2853.](https://www.mdpi.com/2075-4418/13/17/2853)

In short, LLMs and Google Scholar have unintentionally created a powerful tool for identifying bad research: one hallucinates references, and the other indexes them.

Researchers eager to integrate LLMs into their workflows or cite references without verifying their legitimacy should be cautious. Blindly trusting citations from other papers—especially without careful revision—risks perpetuating scientific misinformation.
